{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from urllib.error import HTTPError\n",
    "from urllib.parse import urlparse\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "source=[]\n",
    "http_error=[]\n",
    "attr_error=[]\n",
    "http_error_source=[]\n",
    "attr_error_source=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#url to crawl\n",
    "url_crawl=\"http://www.logitech.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getLinks(pageUrl):\n",
    "    global source_pages\n",
    "    try:\n",
    "        html = urlopen(url_crawl+pageUrl)\n",
    "    except HTTPError as e:\n",
    "        http_error.append(pageUrl)\n",
    "        #print(e)\n",
    "        return None\n",
    "    try:\n",
    "        bsObj = BeautifulSoup(html,\"lxml\")\n",
    "    except AttributeError as e:\n",
    "        attr_error.append(pageUrl)\n",
    "        print(e)\n",
    "        pass\n",
    "    for link in bsObj.findAll(\"a\",href=re.compile(\"^/\")):\n",
    "         if 'href' in link.attrs:\n",
    "                if link.attrs['href'] not in source_pages:\n",
    "                    newPage = link.attrs['href']\n",
    "                    source_pages.append(newPage)\n",
    "                    getLinks(newPage)\n",
    "getLinks(\"\")\n",
    "\n",
    "pd.DataFrame(source_pages).to_csv('source.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Retrieves a list of all Internal links found on a page\n",
    "def getInternalLinks(bsObj, includeUrl):\n",
    "    includeUrl = urlparse(includeUrl).scheme+\"://\"+urlparse(includeUrl).netloc\n",
    "    internalLinks = []\n",
    "    #Finds all links that begin with a \"/\"\n",
    "    for link in bsObj.findAll(\"a\", href=re.compile(\"^(/|.*\"+includeUrl+\")\")):\n",
    "        if link.attrs['href'] is not None:\n",
    "            if link.attrs['href'] not in internalLinks:\n",
    "                if(link.attrs['href'].startswith(\"/\")):\n",
    "                    internalLinks.append(includeUrl+link.attrs['href'])\n",
    "                else:\n",
    "                    internalLinks.append(link.attrs['href'])\n",
    "    return internalLinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "links_df=[]\n",
    "links_df=pd.DataFrame(columns=['source','target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for pp in source_pages:\n",
    "    source=url_crawl + pp\n",
    "    try:\n",
    "        html = urlopen(source)\n",
    "    except HTTPError as e:\n",
    "        pass    \n",
    "    bsObj = BeautifulSoup(html,\"lxml\")\n",
    "    int_links_list=getInternalLinks(bsObj, includeUrl=url_crawl)\n",
    "    for i in range(len(int_links_list)):\n",
    "        source_temp=source\n",
    "        target_temp=int_links_list[i]\n",
    "        temp0=pd.DataFrame({\n",
    "            'source'       : [source_temp],\n",
    "            'target'        : [target_temp]})\n",
    "        links_df=links_df.append(temp0)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "links_df.to_csv('source_target.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
